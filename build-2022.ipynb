{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976c6b8a",
   "metadata": {},
   "source": [
    "# Making sense of the world through vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88150b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer Vision\n",
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "# Face API\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from azure.cognitiveservices.vision.face.models import TrainingStatusType, Person\n",
    "\n",
    "# Speech API\n",
    "from azure.cognitiveservices.speech import AudioDataStream, SpeechConfig, SpeechSynthesizer, SpeechSynthesisOutputFormat\n",
    "from azure.cognitiveservices.speech.audio import AudioOutputConfig\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# Other\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import requests, uuid, json\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from PIL import Image #, ImageDraw\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as IPythonImage\n",
    "from IPython.display import Audio as IPythonAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e1888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login into our Azure Subscription\n",
    "# az login --use-device-code\n",
    "\n",
    "# Create a resource group\n",
    "! az group create -n Build_2022_RG -l westeurope --output table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f4be3",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "#! az cognitiveservices account list-kinds --output table\n",
    "\n",
    "! az cognitiveservices account create \\\n",
    "    --name Build22-CognitiveEndpoint \\\n",
    "    --resource-group Build_2022_RG \\\n",
    "    --kind CognitiveServices \\\n",
    "    --sku S0 \\\n",
    "    --location westeurope \\\n",
    "    --yes \\\n",
    "    --output table\n",
    "\n",
    "! az cognitiveservices account keys list \\\n",
    "    --name Build22-CognitiveEndpoint --resource-group Build_2022_RG \\\n",
    "    --query key1 > key.txt\n",
    "\n",
    "!  az cognitiveservices account show \\\n",
    "    --name Build22-CognitiveEndpoint --resource-group Build_2022_RG \\\n",
    "    --query properties.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d3c53",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "with open('key.txt') as f:\n",
    "    key = f.readlines()\n",
    "\n",
    "subscription_key =  (key[0].replace(\"\\\"\",\"\")).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f89d47",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://westeurope.api.cognitive.microsoft.com/\" \n",
    "\n",
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97958ead",
   "metadata": {},
   "source": [
    "## Describe what is on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"images/amsterdam-gaa24fa0bd_1280.jpg\"\n",
    "display(IPythonImage(filename=image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244875da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(image_url), \"rb\") as image_stream:\n",
    "    description_results = computervision_client.describe_image_in_stream(image_stream)\n",
    "\n",
    "    for description in description_results.captions:\n",
    "        print(description.text, \"| Confidence: \",\"%.2f\" % description.confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb1315",
   "metadata": {},
   "source": [
    "## Detect what is on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect objects in the Images\n",
    "with open(os.path.join(image_url), \"rb\") as image_stream:\n",
    "    detect_objects_results_remote = computervision_client.detect_objects_in_stream(image_stream)\n",
    "\n",
    "    im = plt.imread(image_url)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig = plt.figure(figsize = (im.shape[1]/80, im.shape[0]/80))\n",
    "    ax = plt.axes((0,0,1,1))\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im,origin='upper')\n",
    "\n",
    "    # Overlay the information\n",
    "    for object in detect_objects_results_remote.objects:\n",
    "        color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "        rect = patches.Rectangle((object.rectangle.x, object.rectangle.y), \n",
    "                                 object.rectangle.w, object.rectangle.h, \n",
    "                                 linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.text(\n",
    "            (1/im.shape[1]*object.rectangle.x), 1-(1/im.shape[0]*object.rectangle.y), object.object_property,\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='bottom',\n",
    "            fontsize=16,\n",
    "            color='w',\n",
    "            backgroundcolor=color,\n",
    "            transform=ax.transAxes\n",
    "        )\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b20054",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get more insights on Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad810844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an authenticated FaceClient.\n",
    "face_api_endpoint = \"https://westeurope.api.cognitive.microsoft.com/\"\n",
    "face_client = FaceClient(face_api_endpoint, CognitiveServicesCredentials(subscription_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_image = \"face-photos/tech-a11y-crew.jpg\"\n",
    "display(IPythonImage(filename=mf_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66801a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(mf_image), \"rb\") as image_stream:\n",
    "    \n",
    "    # Detect faces in images\n",
    "    detected_faces = face_client.face.detect_with_stream(image_stream, return_face_attributes=[\n",
    "                    'age', \n",
    "                    'smile',\n",
    "                    'facialHair',\n",
    "                    'glasses',\n",
    "                    'emotion',\n",
    "                    'hair',\n",
    "                    'accessories'\n",
    "                ])\n",
    "    \n",
    "    # Display the result\n",
    "    pil_img = Image.open(mf_image)\n",
    "    for face in detected_faces: \n",
    "        img2 = pil_img.crop((face.face_rectangle.left, face.face_rectangle.top, face.face_rectangle.left+face.face_rectangle.width, face.face_rectangle.top+face.face_rectangle.height))\n",
    "        display(img2)\n",
    "        print (f'Face id: {face.face_id}')\n",
    "        print (f'smile: {face.face_attributes.smile}')\n",
    "        print (f'age: {face.face_attributes.age}')\n",
    "        print (f'facial_hair moustache: {face.face_attributes.facial_hair.moustache}')\n",
    "        print (f'facial_hair beard: {face.face_attributes.facial_hair.beard}')\n",
    "        print (f'facial_hair sideburns: {face.face_attributes.facial_hair.sideburns}')\n",
    "        print (f'glasses: {face.face_attributes.glasses}')\n",
    "        print (f'emotion: {face.face_attributes.emotion}')\n",
    "        print(\" ==\")\n",
    "\n",
    "    print()\n",
    "\n",
    "# Save this ID for use in Find Similar\n",
    "first_image_face_ID = detected_faces[0].face_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24188115",
   "metadata": {},
   "source": [
    "## Train the face API to recognize people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eddb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the dataset\n",
    "path = r\"face-photos/train\"\n",
    "random_filenames = []\n",
    "for train_img in os.listdir(path):\n",
    "    random_filenames.append(os.path.join(path, train_img))\n",
    "\n",
    "grid = AxesGrid(plt.figure(1, (20,20)), 111, nrows_ncols=(1, 6), axes_pad=0, label_mode=\"1\")\n",
    "\n",
    "i = 0\n",
    "for img_name in random_filenames:\n",
    "    im = plt.imread(img_name)\n",
    "    grid[i].imshow(im,aspect='auto', extent=(0,0.8,0,1), alpha=1, origin='upper', zorder=-1)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a face group\n",
    "PERSON_GROUP_ID = \"tech-a11y-crew\"\n",
    "face_client.person_group.delete(person_group_id=PERSON_GROUP_ID)\n",
    "face_client.person_group.create(person_group_id=PERSON_GROUP_ID, name=PERSON_GROUP_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add persons and face photos to the group\n",
    "path = r\"face-photos/train\"\n",
    "\n",
    "for person in os.listdir(path):\n",
    "    name = person.partition(\".\")[0]\n",
    "    print(\"Adding:\"+name)\n",
    "    w = open(os.path.join(path,person), 'r+b')\n",
    "\n",
    "    # Create a person\n",
    "    person = face_client.person_group_person.create(PERSON_GROUP_ID, name)\n",
    "\n",
    "    # Add a face to the person\n",
    "    face_client.person_group_person.add_face_from_stream(PERSON_GROUP_ID, person.person_id, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the person group\n",
    "face_client.person_group.train(PERSON_GROUP_ID)\n",
    "\n",
    "while (True):\n",
    "    training_status = face_client.person_group.get_training_status(PERSON_GROUP_ID)\n",
    "    print(\"Training status: {}.\".format(training_status.status))\n",
    "    if (training_status.status is TrainingStatusType.succeeded):\n",
    "        break\n",
    "    elif (training_status.status is TrainingStatusType.failed):\n",
    "        face_client.person_group.delete(person_group_id=PERSON_GROUP_ID)\n",
    "        sys.exit('Training the person group has failed.')\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7315576",
   "metadata": {},
   "source": [
    "## Identify people in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44506a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect faces\n",
    "with open(os.path.join(mf_image), \"rb\") as image_stream:\n",
    "    # Detect faces\n",
    "    face_ids = []\n",
    "    # We use detection model 3 to get better performance.\n",
    "    faces = face_client.face.detect_with_stream(image_stream, detection_model='detection_03')\n",
    "    for face in faces:\n",
    "        face_ids.append(face.face_id)\n",
    "\n",
    "# Indentify faces\n",
    "results = face_client.face.identify(face_ids, PERSON_GROUP_ID)\n",
    "\n",
    "identified_persons = {}\n",
    "\n",
    "for person in results:\n",
    "    for candidate in person.candidates:\n",
    "        identified_person = face_client.person_group_person.get(PERSON_GROUP_ID,candidate.person_id)\n",
    "        print(\"Found: \"+identified_person.name)\n",
    "        identified_persons[person.face_id] = identified_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ade76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the result\n",
    "\n",
    "im = plt.imread(mf_image)\n",
    "\n",
    "# Create figure and axes\n",
    "fig = plt.figure(figsize = (im.shape[1]/70, im.shape[0]/70))\n",
    "ax = plt.axes((0,0,1,1))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im,origin='upper')\n",
    "\n",
    "# Overlay the information\n",
    "for face in faces:\n",
    "    color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "    rect = patches.Rectangle((face.face_rectangle.left, face.face_rectangle.top), \n",
    "                             face.face_rectangle.width, face.face_rectangle.height, \n",
    "                             linewidth=3, edgecolor=color, facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    if face.face_id in identified_persons:\n",
    "        ax.text(\n",
    "            (1/im.shape[1]*face.face_rectangle.left), 1-(1/im.shape[0]*face.face_rectangle.top), \n",
    "            \"{}\".format(identified_persons[face.face_id].name),\n",
    "            horizontalalignment='left', verticalalignment='bottom', fontsize=16, color='w', backgroundcolor=color, transform=ax.transAxes\n",
    "        )\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce94dc3",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Read text in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "handwriting_image_url = \"images/text.png\"\n",
    "display(IPythonImage(filename=handwriting_image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== Start =====\")\n",
    "# Call API with URL and raw response (allows you to get the operation location)\n",
    "with open(os.path.join(handwriting_image_url), \"rb\") as image_stream:\n",
    "    read_response = computervision_client.read_in_stream(image_stream,  raw=True)\n",
    "\n",
    "read_operation_location = read_response.headers[\"Operation-Location\"]\n",
    "# Grab the ID from the URL\n",
    "operation_id = read_operation_location.split(\"/\")[-1]\n",
    "\n",
    "# Call the \"GET\" API and wait for it to retrieve the results \n",
    "while True:\n",
    "    read_result = computervision_client.get_read_result(operation_id)\n",
    "    if read_result.status not in ['notStarted', 'running']:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "for text_result in read_result.analyze_result.read_results:\n",
    "    for line in text_result.lines:\n",
    "        print(line.text)\n",
    "\n",
    "print(\"===== Done =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imread(handwriting_image_url)\n",
    "\n",
    "# Create figure and axes\n",
    "fig = plt.figure(figsize = (im.shape[1]/100, im.shape[0]/100))\n",
    "ax = plt.axes((0,0,1,1))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im,origin='upper')\n",
    "\n",
    "full_text = \"\"\n",
    "for text_result in read_result.analyze_result.read_results:\n",
    "    for line in text_result.lines:\n",
    "        color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "        rect = patches.Rectangle((line.bounding_box[0], line.bounding_box[1]), \n",
    "                             line.bounding_box[2]-line.bounding_box[0], line.bounding_box[5]-line.bounding_box[1], \n",
    "                             linewidth=6, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        full_text+=line.text + \" \"\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fbed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = SpeechConfig(subscription=subscription_key, region=\"westeurope\")\n",
    "speech_config.speech_synthesis_language = \"en-GB\" \n",
    "speech_config.speech_synthesis_voice_name =\"en-GB-LibbyNeural\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file=f'{speech_config.speech_synthesis_voice_name}.wav'\n",
    "audio_config = AudioOutputConfig(filename=audio_file)\n",
    "synthesizer = SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "synthesizer.speak_text(full_text)\n",
    "\n",
    "IPythonAudio(audio_file,autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
    "\n",
    "# Add your location, also known as region. The default is global.\n",
    "# This is required if using a Cognitive Services resource.\n",
    "location = \"westeurope\"\n",
    "\n",
    "path = '/translate'\n",
    "constructed_url = endpoint + path\n",
    "\n",
    "params = {\n",
    "    'api-version': '3.0',\n",
    "    'from': 'en',\n",
    "    'to': ['nl', 'ar', 'af']\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "    'Ocp-Apim-Subscription-Region': location,\n",
    "    'Content-type': 'application/json',\n",
    "    'X-ClientTraceId': str(uuid.uuid4())\n",
    "}\n",
    "\n",
    "# You can pass more than one object in body.\n",
    "body = [{\n",
    "    'text': full_text\n",
    "}]\n",
    "\n",
    "request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
    "response = request.json()\n",
    "\n",
    "for translation in response[0]['translations']:\n",
    "    print(translation['text'])\n",
    "\n",
    "#print(json.dumps(response, sort_keys=True, ensure_ascii=False, indent=4, separators=(',', ': ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2678845",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text = response[0]['translations'][2]['text']\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b8c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = SpeechConfig(subscription=subscription_key, region=\"westeurope\")\n",
    "speech_config.speech_synthesis_language = \"af-ZA\" \n",
    "speech_config.speech_synthesis_voice_name = \"af-ZA-AdriNeural\"\n",
    "\n",
    "audio_file=f'{speech_config.speech_synthesis_voice_name}.wav'\n",
    "audio_config = AudioOutputConfig(filename=audio_file)\n",
    "synthesizer = SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "synthesizer.speak_text(translated_text)\n",
    "\n",
    "IPythonAudio(audio_file,autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d748053d0416c9e39ca985898d132dcd9e79dffea847c0937d7c8df10aff9d57"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('demos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
